# 日志结构文件系统块访问深度预测

因为日志结构（Log-structure）文件块较为碎片化地分布在文件系统中，所以在文件的read上会带来比较多寻道操作，从而提升了延迟和开销。

而文件系统块访问的深度预测就可以很好地提前读取要访问的文件块，这样子就可以隐藏访问延迟。

本文的相关依据文献在[日志文件系统（Log-structured File Systems）读优化调研](http://blog.leanote.com/post/454858191@qq.com/日志结构文件系统（Log-structured-File-Systems）调研)这篇文章中。

## 1、获取trace

在访问深度的预测上，我们需要使用使用已有的访问历史记录。有一个网站可以让我们查到block级别的访问记录：

[SNIA IOTTA Repository BLOCK I/O TRACES](http://iotta.snia.org/tracetypes/3)

我们使用的是一个来自于Cambridge的trace集，也就是`MSR-Cambridge`，我们使用里面的`mds_0`。得到的是一个csv文件，根据Readme的内容，这个csv文件一共分7列，每一列包含下面几个内容：

> The files are gzipped csv (comma-separated text) files. The fields in
> the csv are:
>
> Timestamp,Hostname,DiskNumber,Type,Offset,Size,ResponseTime
>
> Timestamp is the time the I/O was issued in "Windows filetime"
> Hostname is the hostname (should be the same as that in the trace file name)
> DiskNumber is the disknumber (should be the same as in the trace file name)
> Type is "Read" or "Write"
> Offset is the starting offset of the I/O in bytes from the start of the logical
> disk.
> Size is the transfer size of the I/O request in bytes.
> ResponseTime is the time taken by the I/O to complete, in Windows filetime
> units.

所以实际上所以我们先要编程对csv文件进行整理，我们只关注read类型，然后因为实际上在csv文件中只提供offset，我们需要设定一个块的大小来将这个csv文件中offset转化为一系列块号。

所以我们我们要写一个程序来来进行现有数据的整理。我们只需要一个序列，那就是操作的访问块号。

而在时序的问题上整个csv文件是按照整个从小到大的顺序排列的。所以就保持现有的顺序就可以了。

我们编写了`select-useful-data`，这个函（文件）可以将所有读的块号读取出来，写到一个新的`block_count`文件当中。

## 2、目的与动力

### 2.1、Linux现有的预测策略

这个章节我们要明确做这个工作的目的，要对比现有传统的预读策略为什么做得不够。

在IBM的技术网站[Linux 内核的文件 Cache 管理机制介绍](https://www.ibm.com/developerworks/cn/linux/l-cache/)中，我们可以看到新的Linux的读缓存管理：

> Linux内核中文件预读算法的具体过程是这样的：对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面(不少于一个页面，通常是三个页面)，这时的预读称为同步预读。对于第二次读请求，如果所读页面不在Cache中，即不在前次预读的group中，则表明文件访问不是顺序访问，系统继续采用同步预读；如果所读页面在Cache中，则表明前次预读命中，操作系统把预读group扩大一倍，并让底层文件系统读入group中剩下尚不在Cache中的文件数据块，这时的预读称为异步预读。无论第二次读请求是否命中，系统都要更新当前预读group的大小。此外，系统中定义了一个window，它包括前一次预读的group和本次预读的group。任何接下来的读请求都会处于两种情况之一：第一种情况是所请求的页面处于预读window中，这时继续进行异步预读并更新相应的window和group；第二种情况是所请求的页面处于预读window之外，这时系统就要进行同步预读并重置相应的window和group。图5是Linux内核预读机制的一个示意图，其中a是某次读操作之前的情况，b是读操作所请求页面不在window中的情况，而c是读操作所请求页面在window中的情况。

我们可以看到，实际上Linux对于文件顺序访问和随机访问是分开考虑的，所以实际上文件的随机访问是没有被处理的。而在Linux现有文件系统中，因为读的开销并不大（通过inode+bitmap就可以了）。所以这种死板的预读策略在顺序读取上有非常良好的表现，但是在随机读写上面实际上因为错误预测的开销比较低，所以在随机访问上的预测错误也是可以接受的。而因为随机访问实际上是比较常见的，所以这种开销就更小了。

### 2.2、顺序访问与随机访问

实际上因为现有文件系统在逻辑地址上面的顺序访问做的已经很不错了，所以对于顺序访问我么是有一定的必要保留的。这里就有一个问题，其实使用基于下文所提出的预测模型实际上也是可以做顺序预测的，但是为什么要将顺序和随机访问分开考虑这个是我考虑的比较多的。

下文我们要提出的所谓是深度预测模型是基于`用户访问模式`来进行预测的。但是预测手段在我看来实际上分为三类：**一种只是不基于用户访问记录**的，比如现有的Linux的算法（上文提及），完全不基于历史的访问记录，就是多顺序读几页，这种算法在顺序访问上有奇效，但是只考虑了单一使用场景。还有一种就是**只看一点点的记录**，比如在文件访问预测比较常用的LS（最近后继）算法，这种算法会根据上一次的同一个文件块的后继来决定这一次相同文件块的后继，这种算法将一定的用户访问模式考虑在内，对于用户的访问模式的变化也有一定的敏感度，对于短期的预测也有比较高的准确性（不管是随机还是顺序访问）。第三种就是非常**基于用户长久的使用记录，进行基于概率的预测**，比如对于用户的所有访问历史进行记录（比如马尔科夫模型）并且不断预测用户最为可能访问的节点，这种方式好访问在于能够比较好地进行长远的预测，但是对于用户访问模式的变化的响应性不足。

所以实际上我们可以看到，实际上看用户的访问历史看得越多的越有利于性能更个性化的长远预测；用户访问历史看得少的方式对于已经确定的某种模式的访问有奇效，并且对于模式的变化有比较好的相应，但是对于长远预测的响应性不足。

基于这种特点，我们需要将顺序和随机读取单独处理，因为逻辑地址的顺序读写是文件系统访问的主要问题，如果我们让基于概率的模型加入，那么这样的模型一定会将顺序访问的情况大量考虑在内，那么这种“过拟合”的风险将会导致在随机访问上的预测不足。

因为日志结构文件系统有比较大的读开销，所以为了保证预测的足够准确性，我们使用了多种预测模型的投票方式来提高预测的准确性。这里就涉及到一个权重的问题。因为不同的预测方法擅长的场景不同，所以我们应该在不同的场景下使用不同的权重。比如，如果Linux传统块预读策略多次成功，那就意味着当前是在连续读取的场景，那么这个时候我们就要大胆地增加预读的页数，并且增加Linux传统的预读策略在投票中的权重。如果Linux传统的预读策略多次失败，那么我们就认为这个是非连续读取的场景，那么这个时候我们就要增加另外两种类型的预测器的权重。

在随机访问的预测中，实际上我们也要考虑两个方面，分别是短期预测和长线预测。实际上，以LS（最近后继）为核心的算法在短期预测上有比较明显的优势，并且对于模式的变化反应比较机敏，而已markov为核心的算法在长远预测上有比较大的优势。所以在短预测与长预测上也要有权限上的区分。

所以说长预测和短预测之间我们需要两种不同的策略。对于短期预测来说，实际上Linux自带的page预测拥有非常不错的效能，但是有时候会在不应该让“page连续预读策略”过于强势，我们需要他在停止的时候停止。或者有时候干脆就不能预测，所以我们引入Noah和page进行牵制。这里有两个问题，“能不能预测”以及“怎么预测”。对于“能不能预测”的问题，Noah因为可以记录稳定性，所以我觉得可以在短期预测“一言堂”，主要不稳定，就不预测。通过这样子来充分保证预测率。然后我们使用投票策略，在每一个位置上需要半数以上的预测器认为可以才可以。这是一种我们先行的一种极为保守的策略。

### 2.3、基于LRU的缓存释放策略

在业内已经有较为成熟的缓存替换方案，即LRU策略（最近最少被使用），其实已经是一种考虑了用户使用模式的缓存算法，因为良好的准确性被使用。

## 3、预测模型的设计与实现

### 3.1、Page顺序预读策略模型

#### 3.1.1、Page顺序预读策略模型的设计

传统的预测方法，一开始预测3位，如果失败就一直预测三位，如果预测成功就顺序六位，并且在成功之后持续六位。

#### 3.1.2、Page顺序预读策略模型的实现

根据预测到那就三位，没有预测到那就六位的思路，我们非常轻易就可以给出实现。

```c
void page_predictor(long now_access, long *predictor_arr, int *size){
    if(predictor_arr == NULL){
        printf("predictor_arr必须在外部分配好空间、\n");
        return;
    }
    
    printf("开始进行预测，now_access = %ld\n", now_access);
    //开始预测，看看之前的预测是不是对的
    //查看之前的预测结果

    int i;

    //要返回的块编号的数量
    int return_num = 3;

    for(i = 0; i < MAX_HISTORY_ARR_SIZE; i++){
        //开始检查上一次预测的记录
        if(history_arr[i] == now_access){
            //这里说明这个块是上次预测到的
            return_num = 6;
            break;
        }
    }

    //初始化历史函数，等待下一次使用
    memset(&history_arr,0,MAX_HISTORY_ARR_SIZE*sizeof(long));

    //如果没有预测到，那就依旧返回三个，如果预测到了那就返回6个
    for(i = 0; i < return_num; i++){
        predictor_arr[i] = now_access + i + 1;
        //重新定义历史预测函数
        history_arr[i] = now_access + i + 1;
    }
    
    *size = return_num; 
    
}
```

我们在`MSR-Cambridge`中进行了测试，分别在1K的数据块大小和4K的数据块大小、1024块的缓冲区大小下分别进行了测试，最终的cache命中率分别是90%和81%。我们尝试降低缓冲区的大小看看情况。在4K的数据块大小和100块的缓冲区大小下，缓存命中率降低到78%，实际上我们看到并没有下降太多，我们看1K数据块大小和100块的缓冲区大小，我们发现命中率实际上还是90%。这是一个令人惊讶的数据。

其实出现这样的原因也比较简单，首先文件块越小，那文件块访问的连续性就越强，所以我们可以认为小的文件块+大的缓冲区是有利于Linux传统的预读策略的。

命中率固然很重要，但是实际上我们也要看看错误率，因为实际上缓存命中率可以通过频繁地进行缓冲块替换来保证，实际上并不能保证预取的错误率。

下面我再做一个测试，看看错误命中的比例大概占多少。关于错误命中的概率我们可以在LRU淘汰上面做文章，在这个上面，我们对所有被LRU淘汰的块进行计数，如果这个块在cache之后被访问了，那就标记一下，我们看看被置换出来的块有多少是没有被访问过的，就可以知道预测的正确率了。

我们在Linux自带的page缓存策略上面计算了缓存的正确率（先缓存三个，然后缓存六个），我们发现实际上在这种方式下错误率达到了35%，实际上这个惩罚就有点大了，我们需要做的工作就是尽可能将整个错误率下降到接近0%。并且我们还需要尽可能保证和Linux原有方案一样的3-6个块的预测深度。

### 3.2、基于Noah的深度预测模型

#### 3.2.1、Noah预测的原理

LS是一种比较简单的预测方式，就是按照一个块号的上次的后继来预测这一次的后继。这是一种比较常见的预测方式，对于用户以及程序访问模式的变化有较好的响应度。但是对于深度预测的准确度就比较低了。

Noah是一种基于LS的预测方式，因为日志结构文件系统的读开销较大，所以对于读预测有比较大的开销，但是因为Noah只有在访问模式比较固定的时候才会做出预测，也就是说，只有连续两次出现相同后继的时候在预测的时候才会使用这个后继。

这里是Noah论文所在的文章中提到的伪代码：

<center>

![](https://ws2.sinaimg.cn/large/006tKfTcly1fnvb1qh5vxj30k40eomxu.jpg)

</center>

Counter是一个计数器，来记录相同的后继连续出现的次数。

#### 3.2.2、Noah预测的实现

在Noah预测中我们需要维护两个东西，一个是当前块上一次访问的实际后继，我们还需要一个数组来记录相同实际后继出现的次数。

Counter的更新规则如下，如果发现新的实际后继和上一次实际后继相同，那对应的计数+1。如果不同，计数置零。所以计数器就是和实际后继记录绑定的。我们可以在实际后继的更新操作中进行Counter计数器的更新。

当Counter节点的计数到达稳定性之上的时候，我们就可以进行预测，预测结果就是上次的实际后继，要不就不预测。

我们在是否预测以及每一位的预测上都给与Noah和page连续预读策略相同的权重，使得缓存的命中率下降到11%，并且预测的错误率下降到24%。相比相比之前的相比之前的91%缓存命中率和35%的错误率，只能说的确达到了降低缓存错误率的目的，但是实在是很不理想。接下来我们单独在noah下进行了测试，此外我们也尝试扩大缓冲池的大小（毕竟没有平凡地缓存置换了，缓存大一点试试）来看看效果。

在单独使用noah，并且加大缓存空间的基础上，命中率为36%，错误率为21%。如果同时使用page和noah，那么缓存命中率就是32%，错误率也是21%。我们可以看出实际上连续页缓存的加入使得整个的命中率变低了，但是错误率基本上没有变。我觉得现在命中率过低主要是因为noah的一个“不预测”策略，有些合理的预测实际上被“不预测”的跳过了。

这里我们降低对于Noah稳定性的需求。然后再看看情况。

## 4、预测模型的组合

实际上因为使用了多个预测器，实际上多个预测器的组合是个问题，所以我们需要为每个预测器都设定一个元数据。而在多个预测器下，我们需要知道“怎么才叫一个预测器预测对了”。也就是说当在上次预测深度的块IO上预测到了这个，那就认为这个块的预测是对的。这可以作为“连续访问”和“随机访问”的依据。